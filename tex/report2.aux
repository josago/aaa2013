\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Method}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Average number of steps needed to catch the prey for different policies, shown for reference purposes. These averages were calculated by simulating one million games.}}{3}}
\newlabel{fig:avg}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Average number of steps needed to catch the prey when the \textit  {Q-Learning} algorithm is run for different combinations of its parameters $\alpha \in [0.1, 1]$ and $\gamma \in [0, 0.9]$. We set $\epsilon = 0.1$ and $Q(s, a) = 15, \forall s, a$ for all instances of the algorithm. The best performance obtained, close to 20, corresponds to $\gamma = 0.4$, $\alpha = 0.2$.}}{3}}
\newlabel{fig:test1}{{2}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Evolution on the average number of steps needed to catch the prey for the \textit  {Q-Learning} algorithm for different combinations of its parameters $\alpha $ and $\gamma $. We set $\epsilon = 0.1$ and $Q(s, a) = 15, \forall s, a$ for all instances of the algorithm. The graphs are plotted against the number of episodes the algorithm was trained with.}}{4}}
\newlabel{fig:test2}{{3}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Evolution on the average number of steps needed to catch the prey for the \textit  {Q-Learning} algorithm for different combinations of $\epsilon $ and the initialization of the $Q$ table. We set $\alpha = \gamma = 0.1$ for all tests.}}{5}}
\newlabel{fig:test3}{{4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces $\epsilon $-greedy versus softmax action selection for the \textit  {Q-Learning} algorithm for various values of $\epsilon $ and $\tau $. We fixed $\alpha = \gamma = 0.1$ and $Q(s, a) = 15, \forall s, a$.}}{6}}
\newlabel{fig:test4}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Performance of the implemented Monte-Carlo control methods for different values of the parameter $\gamma $. On-policy is shown on the left, while off-policy is shown on the right plots. $Q(s, a) = 15, \forall s, a$ is shown above and $Q(s, a) = 0, \forall s, a$ is shown in the second row of plots. We fixed $\epsilon = 1$ for all cases.}}{7}}
\newlabel{fig:test5}{{6}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Comparison of the performance of \textit  {Q-Learning} versus \textit  {Sarsa}. \textit  {Q-Learning} is shown on the left, while \textit  {Sarsa} is shown on the right plots. $Q(s, a) = 15, \forall s, a$ is shown above and $Q(s, a) = 0, \forall s, a$ is shown in the second row of plots. We fixed $\epsilon = 1$ for all cases.}}{8}}
\newlabel{fig:test6}{{7}{8}}
